---
title: "Code for WEEK 2: Importing and manipulating data"
output: html_notebook
author: "Jon Sadler"
Date: "20th Jan 2020"
---
Several Data sets for this workshop are derived from Zuur et al. Beginner's Guide to R.

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

NOTE: R notebooks allow you to integrate code into comments and when you run the code the output is added into the notebook.
The code sits in within characters telling R to run the snippets as code:

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

You can short cut these using optionv+vcmdv+vI as a short cut in Mac OS, and

Ctrl + Alt + I in windows

Try it...


*******************************************
PART 1: Getting Data into R via file import 
*******************************************

REMEMBER TO MAKE SURE YOU HAVE SET YOUR WORKING DIRECTORY!

We'll start by using the read.table function for "tab delineated" data
```{r}
Squid <- read.table(file = "squid.txt", header = TRUE)
```

Data input from textfiles and excel spreadsheets and choose.file() 
Make sure you have downloaded the datasets you need from the website! You need squid.txt, deer.txt, compensation.csv
Put them in your workspace!

We strongly recommend that you use the names, str, head, tail and dim functions after import
You need to be sure you've got the correct dataset!!

```{r}
names(Squid)        # Lists the column (or variable names)
str(Squid)          # Indicates the structure of the data frame (i.e. variable types etc)
head(Squid)         # Lists the first 6 rows of data
tail(Squid)         # No prizes for guessing what that does....
dim(Squid)          # Lists the dimensions of the data frame (in this case 2644 cases or rows and 6 variables or columns)
```

Importing an Excel (or any other spreadsheet) CVS file (comma-delineated file)
Make sure your data are formatted correctly with variables in columns. Use sensible column names
And don't use these characters are R doesn't like them: Â£, $, %, ^, ,, ., <, >, ?, \, |, [], {} etc
Make sure missing values have a 'NA' and they are not left blank

```{r}
Comp.dat <- read.csv(file = "compensation.csv", header = TRUE)  
names(Comp.dat)
str(Comp.dat)         # We'll come back to this one in part 3
head(Comp.dat)
Comp.dat               # Lists the whole file
```

For the super lazy or very efficient you can seek a CSV data file using the following:
Try it and import the data file called "ozone.csv" 

```{r}
Ozone <-read.csv(file.choose(), header=TRUE)
names(Ozone)
str(Ozone)
head(Ozone)
```
Beauty of this is that your data files don't have to sit in your working directory 
If you use this command though remember to annotate into your R script where the data file is and its name
You'll forget otherwise!!!

******************************************************
PART 2: Transforming data - Summarising, aggregating *
        filtering, mutating                          *
******************************************************
The next two sections will introduce you to important functions:
tapply(),  table(), in base R
aggregate(), summarise(), mutate() (in dplyr)
AND gather(), spread() and separate(), in tidyR
You'll need a different dataset for this "Vegetation.csv
This data file holds data on vegetation transects in a multi-year study in the US (further information in Zuur et al.)
We'll import this one with the read.csv file.choose() function
You'll need to install the following package (R will install their dependencies):
Tidyverse (it has ggplot, dplyr, tidyr and broom - brilliant packages!)

You'll almost certainly need to install these. We'll use code to do this:

```{r}
install.packages("tidyverse") # This is Hadley Wickham's famous suite of programs, we'll come to them later...
library(tidyverse)
```

As always we could have used on line: install.packages(c("dplyr", "tidyr")) to load the components not the full suite

Download datafile for work (It is called 'Vegetation.csv')

```{r}
Veg <- read.csv(file.choose(), header=TRUE)
names(Veg)
str(Veg)
```
The tapply function to create summaries of your variables (means, sd etc)
Let's say you're interested in finding how the mean species richness (R in the data file) differs across transects 8 transects - a lot of effort. The unique function shows you how many unique levels a factor has:

```{r}
unique(Veg$Transect)
```

Fortunately, one function will do that for you:

```{r}
tapply(Veg$R, Veg$Transect, mean)       
```
or alternative syntax: 
```{r}
tapply(X = Veg$R, INDEX = Veg$Transect, FUN = mean)
```
Used when you are looking for functions on subsets of a variable conditional on another variable generally a factor
You can use any arithmetic function sd, median var (variance)

```{r}
Me <- tapply(Veg$R, Veg$Transect, mean)    # Calculates the mean
Sd <- tapply(Veg$R, Veg$Transect, sd)      # Calculates the sd
Le <- tapply(Veg$R, Veg$Transect, length)  # Calculates the number of observations (length)
Descriptors <- cbind(Me, Sd, Le)
Descriptors
```

This task is much easier using another package dplyr (from Wickham's tidyverse package). The code is simpler and package super powerful.
Key functions in this package are:
select() - allows you to select columns in a dataframe for further manipulation
filter() - subsetting or removing observations based on some condition.
group_by() - groups variables by columns
summarise() - provides summary statistics for a group of observations (e.g. mean, SD etc)
arrange() - orders dataframes by observations (it's a sorting function)
join() - allows dataframes to be joined using unique attribute IDs
mutate() - create new columns on the basis of calculations and transforms of other columns

```{r}
require(dplyr)
```

So let's repeat the above operation with dplyr. It can be done with one line of code.

```{r}
summarise(group_by(Veg, Transect), mean=mean(R), sd=sd(R), length=length(R))
```

Big plus here is that the result of this is saved as a dataframe if we pipe it into an object

```{r}
Descriptors <- summarise(group_by(Veg, Transect), mean=mean(R), sd=sd(R), length=length(R))
```

You can also use the piping operators (%>%) to build highly complex wrangles. Here's the one we've just done...
We'll revisit this idea later with a different datasets.

```{r}
Veg %>% group_by(Transect) %>% summarise(mean=mean(R), sd=sd(R), length=length(R))
```

I like this code. It basically says. Take the veg dataframe, group observations by transect, then
provide the mean, standard deviation and length of each transect. If you want to store it in an object then just add Descriptors <-  before the code. This will then give you a dataframe! Double bonus!!

Filtering allows you to subset data. You've already done this the hard way already last session. dplyr makes it easier and quicker.

```{r}
filter(Veg, Transect == 1:2) # displays all the records in transects 1 and 2
```
So you can see that by using combinations of these commands you can pull out, summarise and analyses subsets of data from large data files very quickly and effectively. For example, we can use the above code and then arrange the data by transect:

```{r}
VegT12 <- filter(Veg, Transect == 1:2)
```

Then sort it in ascending order:
```{r}
arrange(VegT12, BARESOIL)     # sorts your vegT12 in ascending order by the amount of baresoil in each quadrat.
```

TRANSFORMing data using the mutate function in dplyr.
We will use the sleep database on mammal sleep time (datafile is: MammalSleep.csv). See http://genomicsclass.github.io/book/pages/dplyr_tutorial.html for metadata

```{r}
sleep <- read.csv(file.choose(), header=TRUE) 
names(sleep)
str(sleep)
```
Mutate and create a new dataframe (sleep1) with a new column showing the proportion of sleep for each species.

```{r}
sleep1 <- sleep %>% 
  mutate(rem_proportion = sleep_rem / sleep_total, 
         bodywt_grams = bodywt * 1000) 
```

Look at your new dataframe with the additional 'new' column.
```{r}
sleep1
```
     

**********************************************************************************************************************
Class exercise - use dplyr to transform R (Richness) within the veg data to the square root (you've done this before)*
**********************************************************************************************************************
Should take a few minutes....
**********************************************************************************************************************

*********************************************
PART 3: Manipulating and tidying data       *
*********************************************

So what is tidy data? Tidy conforms to the following simple rules:
1. One column per variable
2. One line per record or observation
3. One observation unit per table (e.g. counts of species in one experiment, measurements of stream water
temperature in a riffle every 15 seconds)
Read chapter 12 in Groelmund and Wickham's book:
http://r4ds.had.co.nz/tidy-data.html#tidy-data-1

Load new library
```{r}
require(tidyr)
```
key functions in this library are:
gather() 
spread()
separate()
unite()

Example of data with multiple variables in one column
Create simple species by plot by sample "untidy" dataset to play with.....

```{r}
Site <- c("Site1", "Site1", "Site1", "Site2", "Site2", "Site2", "Site3","Site3","Site3")
Count <- c(123, 156, 243, 533, 45, 4, NA, 45, 345)
Sample <- c("S1", "S1", "S1", "S2", "S2", "S2","S3", "S3", "S3")
Plot <- c("P1", "P1", "P1", "P2", "P2", "P2","P3", "P3", "P3")
Species <- c("Sp1", "Sp2", "Sp3", "Sp1", "Sp2", "Sp3","Sp1", "Sp2", "Sp3")
```
Create a dataframe from the vectors you've just created

```{r}
mydata <- data.frame(Site, Plot, Sample, Species, Count)
```

```{r}
View(mydata)
```

This is row (observation) variable type data. Samples and species are all in their own columns with a count.

Now wrangle it so the species are assembled in columns (called Sp1 to Spn") and the count is each cell

SPREADING DATA:
```{r}
MyColumnVarData <- spread(mydata, Species, Count)
MyColumnVarData
```
Certain plot and analytical functions expect to see data in this format....so it's useful to know how to use it.
This is the equivalent of pivot tables in Excel (but with knobs on!)

GATHERING DATA:
gather() will push the data back to a longer format.

```{r}
MyRowVardata <- MyColumnVarData %>%
  gather(Species, Count, c(Sp1, Sp2, Sp3))
MyRowVardata
```
SEPARATING DATA FIELDS:
using separate() and other functions to tidy a messy dataset. We will use a partial datafile on stream temperatures to illustrate this process

Load in the datafile
```{r}
streamT<-read.csv("streamT.csv", header=TRUE, check.names=FALSE) 
```

check.names=FALSE overrides R's formatting for column headings. It would bomb and complain about the headers without it.
Column 1 is distance downstream in a river (m)
Column 2 is depth in the stream bed (m)
Columns 3:30 are dates and times of temperatures readings with T in (C)

Have a look at these data - identify the issues.
```{r}
str(streamT) 
head(streamT)
```

Several of the columns are names are dates not numeric variables
Stream temperature is hidden under the date columns making it difficult to analyse

We are Rgoing to reformat the file so date is in one column with the relevant temperature reading per distance and depth.

```{r}
streamTTidy <- gather(streamT, DateT, Temp, 3:29) 
```

The code gathers columns 3:29 and places the former column headers into a DateT column and the temperatures into a Temp column
The remaining issue is that the DateT column has two variables date and time. We can resolve this using separate()

```{r}
StreamTTidier <- streamTTidy %>%
  separate(DateT, into = c("Date", "Time"), sep = "\\ ") %>%
  separate(Date, into = c("Day", "Month", "Year"), sep = "\\/") %>%
  separate(Time, into = c("Hour", "Min"), sep = "\\:")
```
Line 1 of this code takes the DateT column and pulls out Date from time using the separator \\ . This is a space.
Line 2 separates Date into Day, Month and Year using the separate \\/
Line 3 separates Time into Hours and Minutes using the separator \\:
Check help file for more information on separators.
But in short R is expecting the character that separates the fields to be given immediately after the \\
You now have a tidy datafile to start analysis temperature change by distance downstream, depth into the stream bed and across time each day and or seasonally.

We could have done it one larger code element using the piping operators to gather then separate

```{r}
StreamTTidier2 <- streamT %>% gather(DateT, Temp, 3:29) %>% 
  separate(DateT, into = c("Date", "Time"), sep = "\\ ") %>%
  separate(Date, into = c("Day", "Month", "Year"), sep = "\\/") %>%
  separate(Time, into = c("Hour", "Min"), sep = "\\:")
```
See lubridate package - examples in code below (McGill) it has DOY (Day of Year) and DOM (Day of Month) functions

******************************************
Data Wrangling Example from Brian McGill *
******************************************

Code derived from Brian McGill's blog/data: www.brianmcgill.org/data_wrangling_example.R
Run in association with his blog posts....i.e. read them at:
https://dynamicecology.wordpress.com/2016/08/22/ten-commandments-for-good-data-management/

Work through the code line by line - it will REALLY help your understanding.....
Some brill code snippets for correcting column names (you've done this) and also sorting typos!
Load the holy trinity of data wrangling packages - thank you Hadley Wickham

```{r}
require(lubridate)
```

STEP #1 - First load in files 
These are the Raw Data files. 
They follow a constellation schema per Commandment #3
raptor_survey.csv/df is the fact table
raptor_temps.csv/temps is a 2nd environmental fact table
raptor_sites.csv/siteinfo is a dimension table (site) for both fact tables
Two of the tables are in row-column form (per Command #2)
But one table is in dimensional form rather than row-column. We will need to fix this bring things in NOT as R factors to start - just numbers or strings options(stringsAsFactors = FALSE)

We have 3 files to load:

```{r}
df <- read.csv(file.choose(),header=TRUE) # raptor_survey.csv
temps <- read.csv(file.choose(),header=TRUE) # raptor_temps.csv
siteinfo <- read.csv(file.choose(),header=TRUE) # raptor_sites.csv
```

STEP #2 - explore what we have - determine dimensions, joining fields
Ccheck for errors. This is part of commandment #4

```{r}
str(df)  #hmm Alpha column/site has "1.o" instead of 1.0 (is chr not int)
head(df)
summary(df)
unique(df$spec)  #hmm Accipiter cooperii misspelled a bit
head(temps)
summary(temps)  #oops - 100 C is a bit hot
unique(temps$site)
head(siteinfo)  #column is "sitename" instead of "site"
unique(siteinfo$site)
summary(siteinfo)
```

STEP #3 - clean up stuff (e.g. column names, things in single columns)
Per commandment #4 we are doing cleaning up
Per command #5 we are doing this in scripts and not hand editing the raw files

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
it is MUCH more repeatable to fix here in script 
and leave raw files untouched
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

3a) site column called "sitename" in siteinfo inconsistent with "site" in core table

```{r}
colnames(siteinfo)[[1]]="site"
str(siteinfo)  #names should match
```
3b) fix string abunds for Alpha site

```{r}
as.numeric(df$Alpha) #can't just coerce - gives warning
df$Alpha #lots of NAs are OK, #'s look OK
unique(df$Alpha[!is.na(df$Alpha)]) #zoom in on non-NAs
```

ah ha somebody typed 1.o instead of 1.0

```{r}
df$Alpha[df$Alpha=="1.o"]=1
```

note other site columns are integer so convert to integer
```{r}
df$Alpha=as.integer(df$Alpha) #no errors - nailed it
str(df) #OK all site columns numeric
```

STEP #4 - Make tall & filter
Per commandment #2 we take our one table that is dimensional and make it row-column

dplyr::gather([data],name_new_factor_column,name_new_value_column,cols to use or exclude)

```{r}
dftall=df %>% gather(site,abund,-date,-spec)
dftall1=dftall %>% filter(abund>0) #drop all the NA rows
```

STEP #5 - more cleanup - again do in code, not raw files
Commandment #4/5 again - I told you it was a large part of the work!

5a fix cooperii misspelling
```{r}
fix=which(dftall1$spec=="Accipiter cooperi")
cat("# of cooperi misspellings=",length(fix))
dftall1[fix,"spec"]="Accipiter cooperii"
unique(dftall1$spec)
```

5b make a genus column
separate(column,newcolnames,separater(default to non-alphanumeric))
```{r}
dftall2= dftall1 %>%separate(spec,c("genus","specname"),remove=FALSE) %>%
  select(-specname)
df=dftall2 #save some typing
```

Commandment #6
We have a fact table (df) with 3 dimensions (date, site, species).
We have another fact table (temps) with 2 dimensions (date, site)
These share date and site dimensions
The site dimension has its own dimension table (siteinfo) with additional site attributes

STEP #6 - join in ancillary tables to make one single 
          "denormalized" data frame
per comamndment #8, this is being done in reproducible code

recall 2 tables to join:
    siteinfo only on site
     temps by site & date
we are now working with factors, so clean up siteinfo

do the first join to siteinfo. Beware - by= is one place you have to use quotes on column names

```{r}
df1=full_join(df,siteinfo,by="site")
head(df1)
str(df1)
```
OK now join in temps on 2 fields (site,date)

```{r}
df2=left_join(df1,temps,by=c("site","date"))
head(df2,n=20) #rows 11 & 12 one site, one date, same temp
```

STEP #7 - now is right shape, make columns right type (should be last step)
Per the spirit of Commandment #7 make this data as easy to use as possible
per comamndment #8, this is being done in reproducible code

```{r}

df2$date=ymd(df2$date)
df2$spec=as.factor(df2$spec)
df2$genus=as.factor(df2$genus)
df2$site=as.factor(df2$site)
df2$region=as.factor(df2$region)
df2$lon=as.numeric(df2$lon)
thedata=df2 #pull up to a name I'll remember for analysis
head(thedata)
str(thedata)
```

STEP #8 - add calculated columns, e.g DOY, Month of year

```{r}
thedata=thedata %>% mutate(doy=yday(date),moy=month(date))
```

STEP #9 - Analyses
Per commandment #9 our analyses are being done in reusable code
Per commandment #8 we are conceptualizing this dimensionally,
          and we are making this table completely denormalized
          and as easy to work with (adding calculated fields like DOY)
          but also per commandment #8 we are leaving this in the native
          form for our analysis tool, R and dplyr, namely row-column data

a) filter to april (full month surveyed) & summarize by doy
```{r}
thedata %>% filter(moy==4) %>% group_by(doy) %>% summarise(abund=mean(abund,na.rm=TRUE))
```

b) same but doy and region
```{r}
thedata %>% filter(moy==4) %>% group_by(doy,region) %>% summarise(abund=mean(abund,na.rm=TRUE)) %>% spread(region,abund)
```

c) top 5 counts
```{r}
thedata %>% top_n(5,abund)
```

d) summarize to views per month, normalized by days observed
```{r}
thedata %>% group_by(moy) %>% summarise(abund=sum(abund,na.rm=TRUE),obsdays=n()) %>% 
  mutate(abund.per.day=abund/obsdays) %>% select(moy,abund.per.day)
```

e) summarise to month  x species table (normalized by days observed)
```{r}
thedata %>% group_by(moy,spec) %>% summarise(abund=sum(abund,na.rm=TRUE),obsdays=n()) %>% 
  mutate(abund.per.day=abund/obsdays) %>% select(moy,spec,abund.per.day) %>%
  spread(moy,abund.per.day)
```

summarize to views per month each year, normalized by days observed
```{r}
thedata %>% group_by(moyr=floor_date(date,"month")) %>% summarise(abund=sum(abund,na.rm=TRUE),obsdays=n()) %>% 
  mutate(abund.per.day=abund/obsdays) %>% select(moyr,abund.per.day)
```


**********************************************************   
CLASS EXERCISES:  Importing and manipulating data frames *
**********************************************************
1. Use the deep sea research data (ISIT.txt). Load it in using read.table. The file contains bioluminscence data oo organisms from various depths and locations in the North Sea. Repeat using tidyverse packages.

2. Extract the data from station 1. How many observations are there from this station?

3. What are minimum, maximum, median and mean sampled depth of stations 2 and 3.

4. Identify stations with fewer observations than 20. Create a data frame omitting them.

5. Extract the data for 2002 and sort it by increasing depth values.

6. Show the data that were measured at depths greater than 2000m in April (all years).

7. Check out the merge() function that allows you to merge datasets with a common ID. 


